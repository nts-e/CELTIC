{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CELTIC Model\n",
    "\n",
    "In this notebook, we demonstrate the process of training the CELTIC model. \n",
    "\n",
    "Using the single cell images and the context data, we initialize the experiment, configure the model, and run the training process. The trained model is saved in a local folder for later use in predictions (see `predict.ipynb`).\n",
    "\n",
    "Please download the microtubules dataset from BIA, as used in this running example.\n",
    "\n",
    "**Important:** Unlike the two other notebook examples on prediction and context creation, this training example requires a large amount of single-cell data that cannot be downloaded inline. The data must be downloaded using an FTP client (e.g., FileZilla) from our BioImage Archive dataset [FTP server](ftp://ftp.ebi.ac.uk/pub/databases/biostudies/S-BIAD/156/S-BIAD2156/Files). Therefore, if you are running this in Google Colab, you must first link the data to your Google Drive. Otherwise, do not run this example in Colab. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CELTIC installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# package installation (e.g for Colab users)\n",
    "!git clone https://github.com/zaritskylab/CELTIC\n",
    "%cd CELTIC\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from celtic.utils.functions import initialize_experiment, download_resources\n",
    "from celtic.train import train\n",
    "import os\n",
    "\n",
    "# Presets\n",
    "organelle = 'microtubules'\n",
    "abs_path_resources_dir = f'/content/CELTIC/resources/{organelle}' # location of the samples to be downloaded\n",
    "\n",
    "# This is the local path to the training images (signal, target, mask).\n",
    "# To be downloaded from from our BioImage Archive dataset, path:microtubules/cell_images. For more details see the notebook header section above.\n",
    "path_single_cells = f'/sise/assafzar-group/assafzar/Nitsan/hipsc_single_cell_image_dataset/{organelle}/fov_processed/cells/source'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the training metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the aria2 download utility\n",
    "!apt-get install -y aria2\n",
    "\n",
    "# !mkdir -p $abs_path_resources_dir\n",
    "bia_ftp_dir = \"ftp://ftp.ebi.ac.uk/pub/databases/biostudies/S-BIAD/156/S-BIAD2156/Files/microtubules/data_for_git_examples/resources/microtubules/metadata\"\n",
    "!aria2c -x 2 -s 2 -c -d  {abs_path_resources_dir} {bia_ftp_dir}/train_images.csv\n",
    "!aria2c -x 2 -s 2 -c -d  {abs_path_resources_dir} {bia_ftp_dir}/train_context.csv\n",
    "!aria2c -x 2 -s 2 -c -d  {abs_path_resources_dir} {bia_ftp_dir}/valid_images.csv\n",
    "!aria2c -x 2 -s 2 -c -d  {abs_path_resources_dir} {bia_ftp_dir}/valid_context.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the Experiment\n",
    "\n",
    "This step initializes the experiment by creating a local folder to store the training files. It also sets up CSV files that contain the paths to the images, and if contexts are used, it includes CSV files with the context data. In this example, we provide the microtubules context files. The process of context creation is explained in the `context_creation.ipynb` notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the experiment will be saved in: ./experiments/train/microtubules/2025-01-11-21-42-40\n"
     ]
    }
   ],
   "source": [
    "path_run_dir, context_model_config = initialize_experiment(organelle, \n",
    "                                                           'train', \n",
    "                                                           models_dir=f'{abs_path_resources_dir}/models')\n",
    "print(\"the experiment will be saved in:\", path_run_dir)\n",
    "\n",
    "path_images_csv = [f'{abs_path_resources_dir}/{item}_images.csv' for item in ['train', 'valid']]\n",
    "path_context_csv = [f'{abs_path_resources_dir}/{item}_context.csv' for item in ['train', 'valid']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Training\n",
    "\n",
    "This step starts the training process using the specified parameters, including image paths, context data, and model configuration. The results are saved in the local folder of the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.run_training(path_run_dir,\n",
    "                    path_images_csv, \n",
    "                    path_context_csv,\n",
    "                    path_single_cells, \n",
    "                    masked = True,\n",
    "                    transforms = context_model_config['transforms'],\n",
    "                    patch_size = context_model_config['train_patch_size'],\n",
    "                    iterations = 60_000,\n",
    "                    batch_size = 24,\n",
    "                    learning_rate = 0.001,\n",
    "                    context_features = context_model_config['context_features'], \n",
    "                    daft_embedding_factor = context_model_config['daft_embedding_factor'], \n",
    "                    daft_scale_activation = context_model_config['daft_scale_activation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
